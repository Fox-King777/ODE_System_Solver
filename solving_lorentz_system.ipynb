{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lorentz System\n",
    "\n",
    "$\\frac{\\mathrm{d}x}{\\mathrm{d}t} = \\sigma (y - x), \\\\[6pt]$\n",
    "\n",
    "\n",
    "$\\frac{\\mathrm{d}y}{\\mathrm{d}t} = x (\\rho - z) - y, \\\\[6pt]$\n",
    "\n",
    "\n",
    "$\\frac{\\mathrm{d}z}{\\mathrm{d}t} = x y - \\beta z.$\n",
    "\n",
    "$σ = 10$, $β = 8/3$ and $ρ = 28$\n",
    "\n",
    "$x(0) = 0$, $y(0) = 1$, $z(0) = 1.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "from autograd import grad, elementwise_grad\n",
    "from matplotlib import pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XYZ_0 = np.array([0, 1, 1.05])\n",
    "SBR = np.array([10, 8 / 3, 28])\n",
    "\n",
    "\n",
    "def lorenz_derivatives(xyz, *, sbr=SBR) -> np.ndarray:\n",
    "    \"\"\"The Lorenz derivatives.\n",
    "\n",
    "    Args:\n",
    "        xyz : array-like, shape (3,)\n",
    "        srb: array-like, shape (3,)\n",
    "\n",
    "    Returns:\n",
    "         xyz_dot : array, shape (3,)\n",
    "            Values of the Lorenz attractor's partial derivatives at *xyz*.\n",
    "    \"\"\"\n",
    "    s, b, r = sbr\n",
    "    x, y, z = xyz\n",
    "    x_dot = s * (y - x)\n",
    "    y_dot = r * x - y - x * z\n",
    "    z_dot = x * y - b * z\n",
    "    return np.array([x_dot, y_dot, z_dot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def elu(z, alpha: float):\n",
    "    return alpha * (np.exp(z) - 1) if z < 0 else z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NN.png \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A neural network class for solving ODEs or system of ODEs.\n",
    "    Attributes:\n",
    "        init_conditions (np.array): Initial conditions for the target function.\n",
    "        derivative (Callable): Function that calculates the target derivative of the neural network.\n",
    "        input_size (int): Size of the input layer.\n",
    "        hidden_sizes (np.array): Array of integers representing the sizes of the hidden layers.\n",
    "        output_size (int): Size of the output layer.\n",
    "        activation_fns (List[Callable]): List of activation functions for each layer.\n",
    "        weights (List[np.array]): List of weights and biases for each layer.\n",
    "\n",
    "    Methods:\n",
    "        init_weights(): Initializes the weights and biases of the neural network.\n",
    "        forward(t: np.array, weights: List[np.array]) -> np.array: Makes a forward pass through the neural network.\n",
    "        mse_loss_function(t: np.array, weights: List[np.array]) -> float: Calculates the mean squared error of the neural network.\n",
    "        trial_solution(t: np.array, weights: List[np.array]) -> np.ndarray: Calculates the trial solution.\n",
    "        trial_grad(t: np.array, weights: List[np.array]) -> np.array: Calculates the gradient of the trial solution with respect to t.\n",
    "        gradient_descent(t: np.array, num_iter: int, learn_rate: float) -> List[np.array]: Runs gradient descent for a given number of iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_conditions: np.array,\n",
    "        derivative: Callable,\n",
    "        input_size: int,\n",
    "        hidden_sizes: np.array,\n",
    "        output_size: int,\n",
    "        activation_fns: List[Callable],\n",
    "    ):\n",
    "        self.init_conditions = init_conditions\n",
    "        self.derivative = derivative\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.activation_fns = activation_fns\n",
    "        self.weights = [None] * (hidden_sizes.shape[0] + 1)  # +1 for the output\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes the weights and biases of the neural network\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # hidden weights and biases\n",
    "        self.weights[0] = np.random.randn(\n",
    "            self.hidden_sizes[0], self.input_size + 1\n",
    "        )  # +1 for the bias\n",
    "        for i in range(1, self.hidden_sizes.shape[0]):\n",
    "            self.weights[i] = np.random.randn(\n",
    "                self.hidden_sizes[i], self.hidden_sizes[i - 1] + 1\n",
    "            )  # +1 for the bias\n",
    "\n",
    "        # output weights and biases\n",
    "        self.weights[-1] = np.random.randn(\n",
    "            self.output_size, self.hidden_sizes[-1] + 1\n",
    "        )  # +1 for the bias\n",
    "\n",
    "    def forward(self, t: np.array, weights: List[np.array]) -> np.array:\n",
    "        \"\"\"Makes a forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "            t: The t vector\n",
    "            weights: The weights and biases of the neural network\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array of the output of the neural network of dim(self.output_size, len(t)).\n",
    "        \"\"\"\n",
    "        num_layers = len(weights)\n",
    "        # row matrix\n",
    "        t = t.reshape(-1, t.size)\n",
    "\n",
    "        z = None\n",
    "        a = t\n",
    "        for i in range(num_layers):\n",
    "            z = np.matmul(weights[i], np.concatenate((np.ones((1, t.size)), a), axis=0))\n",
    "            a = self.activation_fns[i](z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def mse_loss_function(self, t: np.array, weights: List[np.array]) -> float:\n",
    "        \"\"\"Calculates the mean squared error of the neural network.\n",
    "\n",
    "        Args:\n",
    "            t: The input vector\n",
    "            weights: The weights and biases of the neural network\n",
    "\n",
    "        Returns:\n",
    "            Mean squared error value\n",
    "\n",
    "        \"\"\"\n",
    "        trial_sol = self.trial_solution(t, weights)\n",
    "        grad_star = self.derivative(t, trial_sol)\n",
    "        grad = self.trial_grad(t, weights)\n",
    "        error = grad_star - grad\n",
    "\n",
    "        return np.linalg.norm(error, \"fro\") / np.sqrt(np.size(error))\n",
    "\n",
    "    def trial_solution(self, t: np.array, weights: List[np.array]) -> np.ndarray:\n",
    "        \"\"\"Calculates the trial solution of the system of ODEs.\n",
    "\n",
    "        Args:\n",
    "            t: The input vector\n",
    "            weights: The weights and biases of the neural network\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array of the trial solution of the system of ODEs\n",
    "            dimension (self.output_size, len(t))\n",
    "        \"\"\"\n",
    "        fp = self.forward(t, weights)\n",
    "        return np.array(\n",
    "            [self.init_conditions[i] + t * fp[i] for i in range(self.output_size)]\n",
    "        )\n",
    "\n",
    "    def elementwise_trial_solution(self, t: np.array, weights: List[np.array], index):\n",
    "        return self.trial_solution(t, weights)[index]\n",
    "\n",
    "    def trial_grad(self, t: np.array, weights: List[np.array]) -> np.array:\n",
    "        \"\"\"Calculates the gradient of the trial solution of the Lorentz System.\n",
    "\n",
    "        Args:\n",
    "            t: The input vector\n",
    "            weights: The weights and biases of the neural network\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array of the gradient of the trial solution of the Lorentz System with\n",
    "            dimension (self.output_size, len(t))\n",
    "        \"\"\"\n",
    "        return np.array(\n",
    "            [\n",
    "                elementwise_grad(self.elementwise_trial_solution, 0)(t, weights, i)\n",
    "                for i in range(self.output_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def gradient_descent(self, t: np.array, num_iter: int, learn_rate: float):\n",
    "        \"\"\"Runs gradient descent for a given number of iterations\n",
    "\n",
    "        Args:\n",
    "            t: The input vector\n",
    "            num_iter: The number of iterations\n",
    "            learn_rate: The learning rate\n",
    "\n",
    "        Returns:\n",
    "            weights: The weights and biases of the neural network after gradient descent\n",
    "        \"\"\"\n",
    "        loss_grad_function = grad(self.mse_loss_function, 1)\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            print(\"iteration: \", i)\n",
    "            print(self.mse_loss_function(t, self.weights))\n",
    "            loss_grad = loss_grad_function(t, self.weights)\n",
    "\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j] = self.weights[j] - learn_rate * loss_grad[j]\n",
    "\n",
    "        return self.weights\n",
    "\n",
    "    def adam(\n",
    "        self, t, grad, num_iters=10000, step_size=0.001, b1=0.9, b2=0.999, eps=10**-8\n",
    "    ):\n",
    "        loss_grad_function = grad(self.mse_loss_function, 1)\n",
    "\n",
    "        m = [np.zeros_like(self.weights[i]) for i in range(len(self.nn_weights))]\n",
    "        v = [np.zeros_like(self.weights[i]) for i in range(len(self.nn_weights))]\n",
    "        mhat = [None] * len(self.weights)\n",
    "        vhat = [None] * len(self.weights)\n",
    "        for i in range(num_iters):\n",
    "            g = loss_grad_function(t, self.weightss)\n",
    "            print(\"iteration: \", i)\n",
    "            print(self.mse_loss_function(t, self.nn_weights))\n",
    "\n",
    "            for j in range(len(self.weights)):\n",
    "                m[j] = (1 - b1) * g[j] + b1 * m[j]  # First  moment estimate.\n",
    "                v[j] = (1 - b2) * (g[j] ** 2) + b2 * v[j]  # Second moment estimate.\n",
    "                mhat[j] = m[j] / (1 - b1 ** (i + 1))  # Bias correction.\n",
    "                vhat[j] = v[j] / (1 - b2 ** (i + 1))\n",
    "                self.weights[j] = self.weights[j] - step_size * mhat[j] / (\n",
    "                    np.sqrt(vhat[j]) + eps\n",
    "                )\n",
    "\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lorenz(init_conditions, sbr):\n",
    "    t = np.arange(0, 10, 0.01)\n",
    "    hidden_sizes = np.array([100, 100, 100])\n",
    "    activation_fns = [tanh] * (len(hidden_sizes)) + [lambda x: x]\n",
    "    lorenz_nn = NeuralNetwork(\n",
    "        init_conditions, lorenz_derivatives, 1, hidden_sizes, 3, activation_fns\n",
    "    )\n",
    "    lorenz_nn.gradient_descent(t, 1000, 0.00001)\n",
    "\n",
    "    return lorenz_nn\n",
    "\n",
    "\n",
    "lorenz_nn = train_lorenz(XYZ_0, SBR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(neural_network: NeuralNetwork, scale=1, dt=0.01):\n",
    "    t = np.arange(0, 10, dt)\n",
    "    res = neural_network.forward(t, neural_network.weights)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.set_xlabel(\"X Axis\")\n",
    "    ax.set_ylabel(\"Y Axis\")\n",
    "    ax.set_zlabel(\"Z Axis\")\n",
    "    ax.plot(res[0], res[1], res[2], lw=1)\n",
    "\n",
    "    xyzs, dxyz_dt = simulate_sode(\n",
    "        t, neural_network.init_conditions, derivative=lorenz_derivatives\n",
    "    )\n",
    "    ax.plot(xyzs[:, 0], xyzs[:, 1], xyzs[:, 2], lw=1)\n",
    "\n",
    "\n",
    "def simulate_sode(t, initial_conditions, derivative) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simulate the system of ODEs over the time interval [0, dt * num_steps].\n",
    "\n",
    "    Args:\n",
    "       dt: The time step\n",
    "       initial_conditions: The initial conditions as a NumPy array of length 3\n",
    "       num_steps: The number of time steps to calculate\n",
    "\n",
    "    Returns:\n",
    "       A tuple of two NumPy arrays: the x, y, and z values, and the derivatives.\n",
    "    \"\"\"\n",
    "    dxyz_dt = np.empty((len(t), 3))\n",
    "    xyzs = np.empty((len(t), 3))\n",
    "    xyzs[0] = initial_conditions\n",
    "\n",
    "    for i in range(len(t) - 1):\n",
    "        dt = t[i + 1] - t[i]\n",
    "        xyzs[i + 1] = xyzs[i] + derivative(xyzs[i]) * dt\n",
    "        dxyz_dt[i] = derivative(xyzs[i])\n",
    "\n",
    "    return xyzs, dxyz_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\", palette=\"pastel\", font=\"DeJavu Serif\")\n",
    "plot(lorenz_nn)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
