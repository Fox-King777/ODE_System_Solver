{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e8dd9-d8fa-43ec-b72f-4e9df9d5257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# The trial solution using the deep neural network:\n",
    "def g_trial(x,params, g0 = 10):\n",
    "    return g0 + x*neural_network(params,x)\n",
    "\n",
    "# The right side of the ODE:\n",
    "def g(x, g_trial, gamma = 2):\n",
    "    return gamma*g_trial\n",
    "\n",
    "# The cost function:\n",
    "def cost_function(P, x):\n",
    "    # Evaluate the trial function with the current parameters P\n",
    "    g_t = g_trial(x,P)\n",
    "\n",
    "    # Find the derivative w.r.t x of the trial function\n",
    "    d_g_t = elementwise_grad(g_trial,0)(x,P)\n",
    "\n",
    "    # The right side of the ODE\n",
    "    func = g(x, g_t)\n",
    "\n",
    "    err_sqr = (d_g_t - func)**2\n",
    "    cost_sum = np.sum(err_sqr)\n",
    "\n",
    "    return cost_sum / np.size(err_sqr)\n",
    "\n",
    "def g_analytic(x, gamma = 2, g0 = 10):\n",
    "    return g0*np.exp(gamma*x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6408317-b58d-4193-bd63-604dd97b437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming one input, hidden, and output layer\n",
    "def neural_network(params, x):\n",
    "    # Find the weights (including and biases) for the hidden and output layer.\n",
    "    # Assume that params is a list of parameters for each layer.\n",
    "    # The biases are the first element for each array in params,\n",
    "    # and the weights are the remaning elements in each array in params.\n",
    "\n",
    "    w_hidden = params[0]\n",
    "    w_output = params[1]\n",
    "    # Assumes input x being an one-dimensional array\n",
    "    num_values = np.size(x)\n",
    "    x = x.reshape(-1, num_values)\n",
    "\n",
    "    # Assume that the input layer does nothing to the input x\n",
    "    x_input = x\n",
    "\n",
    "    ## Hidden layer:\n",
    "\n",
    "    # Add a row of ones to include bias\n",
    "    x_input = np.concatenate((np.ones((1,num_values)), x_input ), axis = 0)\n",
    "\n",
    "    z_hidden = np.matmul(w_hidden, x_input)\n",
    "    x_hidden = sigmoid(z_hidden)\n",
    "\n",
    "    ## Output layer:\n",
    "\n",
    "    # Include bias:\n",
    "    x_hidden = np.concatenate((np.ones((1,num_values)), x_hidden ), axis = 0)\n",
    "\n",
    "    z_output = np.matmul(w_output, x_hidden)\n",
    "    x_output = z_output\n",
    "\n",
    "    return x_output\n",
    "\n",
    "def solve_ode_neural_network(x, num_neurons_hidden, num_iter, lmb):\n",
    "    ## Set up initial weights and biases\n",
    "\n",
    "    # For the hidden layer\n",
    "    p0 = npr.randn(num_neurons_hidden, 2 )\n",
    "\n",
    "    # For the output layer\n",
    "    p1 = npr.randn(1, num_neurons_hidden + 1 ) # +1 since bias is included\n",
    "\n",
    "    P = [p0, p1]\n",
    "\n",
    "    print('Initial cost: %g'%cost_function(P, x))\n",
    "\n",
    "    ## Start finding the optimal weights using gradient descent\n",
    "\n",
    "    # Find the Python function that represents the gradient of the cost function\n",
    "    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer\n",
    "    cost_function_grad = grad(cost_function,0)\n",
    "\n",
    "    # Let the update be done num_iter times\n",
    "    for i in range(num_iter):\n",
    "        # Evaluate the gradient at the current weights and biases in P.\n",
    "        # The cost_grad consist now of two arrays;\n",
    "        # one for the gradient w.r.t P_hidden and\n",
    "        # one for the gradient w.r.t P_output\n",
    "        cost_grad =  cost_function_grad(P, x)\n",
    "\n",
    "        P[0] = P[0] - lmb * cost_grad[0]\n",
    "        P[1] = P[1] - lmb * cost_grad[1]\n",
    "\n",
    "    print('Final cost: %g'%cost_function(P, x))\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc7379-6583-41e8-86da-c853925506ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed such that the weight are initialized\n",
    "# with same weights and biases for every run.\n",
    "npr.seed(15)\n",
    "\n",
    "## Decide the vales of arguments to the function to solve\n",
    "N = 50\n",
    "x = np.linspace(0, 1, N)\n",
    "\n",
    "## Set up the initial parameters\n",
    "num_hidden_neurons = 10\n",
    "num_iter = 10000\n",
    "lmb = 0.001\n",
    "\n",
    "# Use the network\n",
    "P = solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)\n",
    "\n",
    "# Print the deviation from the trial solution and true solution\n",
    "res = g_trial(x,P)\n",
    "res_analytical = g_analytic(x)\n",
    "\n",
    "print('Max absolute difference: %g'%np.max(np.abs(res - res_analytical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b29bf-8d8b-4600-9483-da342b3e2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Plot the results\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    plt.plot(x, res_analytical)\n",
    "    plt.plot(x, res[0,:])\n",
    "    plt.legend(['analytical','nn'])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('g(x)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10f9bf-13fe-4aba-8a76-41d1c92fe40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.utils.data as data, torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Learner(L.LightningModule):\n",
    "    def __init__(self, model, init_condition):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.init_condition = init_condition\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x);\n",
    "    \n",
    "    def g(self, g_trial, gamma = 2):\n",
    "        return gamma * g_trial\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        g_trial = self.init_condition + x * self.forward(x)\n",
    "        trial_sol = self.g(g_trial)\n",
    "        #d_g_trial = elementwise_grad(g_trial, 0)(x)\n",
    "        #d_g_trial = grad(outputs=g_trial, inputs=x)\n",
    "        #g_trial.backward(retain_graph=True)\n",
    "        #d_g_trial = -x.grad\n",
    "        d_g_trial = torch.autograd.functional.jacobian(lambda t: self.init_condition + t * self.model(t), x).flatten()\n",
    "        \n",
    "        loss = nn.MSELoss()(d_g_trial, trial_sol)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26c52c-9bab-4745-968d-10a1378aee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, init, end, n):\n",
    "\n",
    "        self.n = n\n",
    "        self.x = torch.linspace(init, end, self.n)\n",
    "        self.y = 10*torch.exp(2*self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.x[idx, np.newaxis]\n",
    "        y = self.y[idx, np.newaxis]\n",
    "\n",
    "        return torch.Tensor(x), torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f5004-4a8c-4fd2-a464-891f61c6cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "net = nn.Sequential(\n",
    "        nn.Linear(1, 20),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(20, 50),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(50, 20),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(20, 1)\n",
    "    )\n",
    "'''\n",
    "\n",
    "net = nn.Sequential(\n",
    "        nn.Linear(1, 10),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(10, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a9ccc-b8a7-42f8-8367-f63162264bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(0, 1, 50)\n",
    "\n",
    "trainer = L.Trainer(logger=False, max_epochs=400)\n",
    "learner = Learner(net, 10)\n",
    "trainer.fit(learner, data.DataLoader(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e8829-ca04-4dfb-8e55-7b1c1553a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = learner.model.forward(train.x.view(50, 1))\n",
    "y = y.detach().cpu()\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(train.x, y, color='black')\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.scatter(train.x, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890748e-2d76-4d3a-bb38-5dae6d620237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
